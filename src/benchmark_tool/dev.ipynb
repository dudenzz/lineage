{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c48d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 693366051319316151\n",
       " xla_global_id: -1]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a65cbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 253990/253990 [00:00<00:00, 407119.52it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "class Edge:\n",
    "    def __init__(self, node1, node2, relationship):\n",
    "        self.node1 = node1\n",
    "        self.node2 = node2\n",
    "        self.relationship = relationship\n",
    "    def __eq__(self, other):\n",
    "        return self.node1.value == other.node1.value and self.node2.value == other.node2.value and self.relationship.name == other.relationship.name \n",
    "    def __hash__(self):\n",
    "        return hash(self.node1.value + self.node2.value + self.relationship.name) \n",
    "    def __str__(self):\n",
    "        return f\"{str(self.node1)} {str(self.relationship)}  {str(self.node2)}\"\n",
    "class Relationship:\n",
    "    \"\"\"Hashable relationship with its name as ID\"\"\"\n",
    "    def __init__(self, name):\n",
    "        self.name = name.strip()\n",
    "    def __eq__(self, n1):\n",
    "        \"\"\"Relationships are equal if they have the same name\"\"\"\n",
    "        return self.name == n1.name \n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "    def __hash__(self):\n",
    "        \"\"\"Relationship name identifies the relationship\"\"\"\n",
    "        return hash(self.name) \n",
    "class Node:\n",
    "    \"\"\"Hashable node with its value as ID\"\"\"\n",
    "    def __init__(self, value):\n",
    "        self.value = value.strip()\n",
    "    def __str__(self):\n",
    "        return self.value\n",
    "    \"\"\"Nodes are equal if they have the same value\"\"\"\n",
    "    def __eq__(self, n1):\n",
    "        return self.value == n1.value\n",
    "    \"\"\"Node value identifies the node\"\"\"\n",
    "    def __hash__(self):\n",
    "        return hash(self.value) \n",
    "class Graph:\n",
    "    \"\"\"Graph stucture\"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the graph\"\"\"\n",
    "        #sets because we need efficient lookups (O(1) for set O(n) for list)\n",
    "        self.nodes = set()\n",
    "        self.relationships = set()\n",
    "        self.edges = []\n",
    "        self.node2id = {}\n",
    "        self.rel2id = {}\n",
    "    def add_triple(self, s_val, o_val, p_val):\n",
    "        \"\"\"Add a single triple to the graph.\"\"\"\n",
    "        #set is comprised of unique element, adding an existing element doesn't affect the set \n",
    "        self.nodes.add(s_val)\n",
    "        self.nodes.add(o_val)\n",
    "        self.relationships.add(p_val)\n",
    "        self.edges.append(Edge(s_val, o_val, p_val))\n",
    "\n",
    "    def finalize(self):\n",
    "        \"\"\"Build indices once after all data is loaded.\"\"\"\n",
    "        self.node2id = {node: i for i, node in enumerate(self.nodes)}\n",
    "        self.rel2id = {rel: i for i, rel in enumerate(self.relationships)}\n",
    "        self.adj = {node: [] for node in self.nodes}\n",
    "        for e in self.edges:\n",
    "            self.adj[e.node1].append((e.node2, e.relationship))\n",
    "    def find_l1_paths(self, node):\n",
    "        pass\n",
    "    def paths2dataset(self):\n",
    "        pass      \n",
    "    def find_paths(self, s, t):\n",
    "        \"\"\"Optimized iterative path finding with max depth 6. This function is slow for our problem, moving implementation to C++ with paralellization.\"\"\"\n",
    "        all_paths = []\n",
    "        # Queue stores: (current_node, current_path, visited_set)\n",
    "        queue = deque([(s, [(s, None)], {s})])\n",
    "        \n",
    "        while queue:\n",
    "            curr_node, path, visited = queue.popleft()\n",
    "            \n",
    "            # Stop if path exceeds max length\n",
    "            if len(path) > 6:\n",
    "                continue\n",
    "            # if len(all_paths) == 3:\n",
    "            #     continue\n",
    "            for neighbor, rel in self.adj.get(curr_node, []):\n",
    "                if neighbor == t:\n",
    "                    # Path length must be > 1 (more than 2 nodes in list)\n",
    "                    if len(path) > 2:\n",
    "                        all_paths.append(path + [(neighbor, rel)])\n",
    "                    continue # Found target, don't need to go deeper from here (simple path)\n",
    "                \n",
    "                if neighbor not in visited and len(path) < 4:\n",
    "                    # Use set union for efficiency in creating the next visited set\n",
    "                    queue.append((neighbor, path + [(neighbor, rel)], visited | {neighbor}))\n",
    "        return all_paths\n",
    "    \n",
    "    def graph2dataset(self):\n",
    "        \"\"\"Convert data to numpy arrays compatible as neural net input\"\"\"\n",
    "        pos_triples = np.array([\n",
    "            [self.node2id[e.node1], self.node2id[e.node2], self.rel2id[e.relationship]] \n",
    "            for e in self.edges\n",
    "        ], dtype=np.int32)\n",
    "        num_pos = len(pos_triples)\n",
    "        num_nodes = len(self.nodes)\n",
    "        num_rels = len(self.relationships)\n",
    "        # We over-sample by 10% to account for accidental \"real\" edges being picked\n",
    "        oversample_factor = 1.1\n",
    "        num_to_sample = int(num_pos * oversample_factor)\n",
    "\n",
    "        neg_subs = np.random.randint(0, num_nodes, num_to_sample)\n",
    "        neg_objs = np.random.randint(0, num_nodes, num_to_sample)\n",
    "        neg_rels = np.random.randint(0, num_rels, num_to_sample)\n",
    "        \n",
    "        neg_triples = np.stack([neg_subs, neg_objs, neg_rels], axis=1)\n",
    "\n",
    "        #Pruning: Filter out samples where sub == obj or triple exists in positive set\n",
    "\n",
    "        def hash_triples(triples):\n",
    "            # Maps (s, o, r) to a single unique integer\n",
    "            return triples[:, 0] * (num_nodes * num_rels) + triples[:, 1] * num_rels + triples[:, 2]\n",
    "\n",
    "        pos_hashes = set(hash_triples(pos_triples))\n",
    "        neg_hashes = hash_triples(neg_triples)\n",
    "        mask = np.array([(h not in pos_hashes) for h in neg_hashes])\n",
    "        mask &= (neg_triples[:, 0] != neg_triples[:, 1])\n",
    "        valid_negatives = neg_triples[mask][:num_pos]\n",
    "        X = np.vstack([pos_triples, valid_negatives])\n",
    "        y = np.concatenate([np.ones(num_pos), np.zeros(len(valid_negatives))])\n",
    "\n",
    "        return X, y\n",
    "            \n",
    "g = Graph()  \n",
    "l_bar = '{desc}: {percentage:.3f}%|'\n",
    "r_bar = '| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, ' '{rate_fmt}{postfix}]'\n",
    "format = '{l_bar}{bar}{r_bar}'\n",
    "data = open('../../SiaILP/data/lineage/test.txt').readlines()  \n",
    "for line in tqdm(data, ncols=100, bar_format=format):\n",
    "    s,p,o = line.split('\\t')\n",
    "    g.add_triple(Node(s), Node(o), Relationship(p))\n",
    "g.finalize()\n",
    "X,y = g.graph2dataset()\n",
    "file = open('triples.data','w+')\n",
    "for edge in X:\n",
    "    file.write(f\"{edge[0]} {edge[1]} {edge[2]}\\n\")\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6f7126af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14287/14287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m609s\u001b[0m 43ms/step - accuracy: 0.9421 - loss: 0.1400 - val_accuracy: 0.9703 - val_loss: 0.1075\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x14d345f53d0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Embedding, Concatenate, Dot, Normalization, Lambda, LSTM, Bidirectional, MaxPooling1D, Flatten, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "\n",
    "num_nodes = len(g.nodes) + 1 \n",
    "num_rels = len(g.relationships) + 1\n",
    "\n",
    "#single input for the triple [s, o, r]\n",
    "triple_input = Input(shape=(3,), name=\"triple_input\")\n",
    "#slice the input\n",
    "s_idx = Lambda(lambda x: x[:, 0])(triple_input)\n",
    "o_idx = Lambda(lambda x: x[:, 1])(triple_input)\n",
    "r_idx = Lambda(lambda x: x[:, 2])(triple_input)\n",
    "#embedding Layers\n",
    "node_emb_layer = Embedding(input_dim=num_nodes, output_dim=300, name=\"Node_Embedding\")\n",
    "rel_emb_layer = Embedding(input_dim=num_rels, output_dim=300, name=\"Rel_Embedding\")\n",
    "s_emb = node_emb_layer(s_idx)\n",
    "o_emb = node_emb_layer(o_idx)\n",
    "r_emb = rel_emb_layer(r_idx)\n",
    "#reshape in order to fit LSTM\n",
    "s_seq = Reshape((1, 300))(s_emb)\n",
    "o_seq = Reshape((1, 300))(o_emb)\n",
    "#add 2 layer bi-directional LSTM\n",
    "lstm_layer_1 = Bidirectional(LSTM(150, return_sequences=True))\n",
    "lstm_layer_2 = Bidirectional(LSTM(150, return_sequences=True))\n",
    "#first LSTM layer\n",
    "fst_lstm_mid = lstm_layer_1(s_seq)\n",
    "scd_lstm_mid = lstm_layer_1(o_seq)\n",
    "#second LSTM layer\n",
    "fst_lstm_fin = lstm_layer_2(fst_lstm_mid)\n",
    "scd_lstm_fin = lstm_layer_2(scd_lstm_mid)\n",
    "#reduce max\n",
    "pool_layer = MaxPooling1D(pool_size=1) \n",
    "fst_pooled = pool_layer(fst_lstm_fin)\n",
    "scd_pooled = pool_layer(scd_lstm_fin)\n",
    "#flatten\n",
    "fst_final = Flatten()(fst_pooled)\n",
    "scd_final = Flatten()(scd_pooled)\n",
    "#merge node embeddings with DNN\n",
    "nodes_concat = Concatenate()([s_emb, o_emb])\n",
    "nodes_representation = Dense(300)(nodes_concat)\n",
    "#normalization\n",
    "rel_normalized = Normalization(axis=-1)(r_emb)\n",
    "nodes_normalized = Normalization(axis=-1)(nodes_representation)\n",
    "#edge probability\n",
    "edge_probability = Dot(axes=-1)([rel_normalized, nodes_normalized])\n",
    "edge_probability = Flatten()(edge_probability)\n",
    "output = Dense(1, activation='sigmoid')(edge_probability)\n",
    "\n",
    "m = Model(inputs = triple_input, outputs = output)\n",
    "m.compile(loss='binary_crossentropy', metrics=['accuracy'])\n",
    "m.fit(X,y,validation_split=0.1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
